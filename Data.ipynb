{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dell\\AppData\\Local\\Programs\\Python\\Python310\\python.exe\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv \n",
    "\n",
    "with open(\"training.1600000.processed.noemoticon.csv\", 'r') as f:\n",
    "    \n",
    "    reader = csv.reader(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"training.1600000.processed.noemoticon.csv\", encoding='latin-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1467810369</th>\n",
       "      <th>Mon Apr 06 22:19:45 PDT 2009</th>\n",
       "      <th>NO_QUERY</th>\n",
       "      <th>_TheSpecialOne_</th>\n",
       "      <th>@switchfoot http://twitpic.com/2y1zl - Awww, that's a bummer.  You shoulda got David Carr of Third Day to do it. ;D</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810672</td>\n",
       "      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>scotthamilton</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810917</td>\n",
       "      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>mattycus</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811184</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>ElleCTF</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811193</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Karoli</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811372</td>\n",
       "      <td>Mon Apr 06 22:20:00 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>joy_wolf</td>\n",
       "      <td>@Kwesidei not the whole crew</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0  1467810369  Mon Apr 06 22:19:45 PDT 2009  NO_QUERY _TheSpecialOne_  \\\n",
       "0  0  1467810672  Mon Apr 06 22:19:49 PDT 2009  NO_QUERY   scotthamilton   \n",
       "1  0  1467810917  Mon Apr 06 22:19:53 PDT 2009  NO_QUERY        mattycus   \n",
       "2  0  1467811184  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY         ElleCTF   \n",
       "3  0  1467811193  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY          Karoli   \n",
       "4  0  1467811372  Mon Apr 06 22:20:00 PDT 2009  NO_QUERY        joy_wolf   \n",
       "\n",
       "  @switchfoot http://twitpic.com/2y1zl - Awww, that's a bummer.  You shoulda got David Carr of Third Day to do it. ;D  \n",
       "0  is upset that he can't update his Facebook by ...                                                                   \n",
       "1  @Kenichan I dived many times for the ball. Man...                                                                   \n",
       "2    my whole body feels itchy and like its on fire                                                                    \n",
       "3  @nationwideclass no, it's not behaving at all....                                                                   \n",
       "4                      @Kwesidei not the whole crew                                                                    "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns = [\"Sentiment\",\"ID\",\"Date\", \"Query\",\"User\",\"Tweet\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>ID</th>\n",
       "      <th>Date</th>\n",
       "      <th>Query</th>\n",
       "      <th>User</th>\n",
       "      <th>Tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810672</td>\n",
       "      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>scotthamilton</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810917</td>\n",
       "      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>mattycus</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811184</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>ElleCTF</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811193</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Karoli</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811372</td>\n",
       "      <td>Mon Apr 06 22:20:00 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>joy_wolf</td>\n",
       "      <td>@Kwesidei not the whole crew</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Sentiment          ID                          Date     Query  \\\n",
       "0          0  1467810672  Mon Apr 06 22:19:49 PDT 2009  NO_QUERY   \n",
       "1          0  1467810917  Mon Apr 06 22:19:53 PDT 2009  NO_QUERY   \n",
       "2          0  1467811184  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "3          0  1467811193  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "4          0  1467811372  Mon Apr 06 22:20:00 PDT 2009  NO_QUERY   \n",
       "\n",
       "            User                                              Tweet  \n",
       "0  scotthamilton  is upset that he can't update his Facebook by ...  \n",
       "1       mattycus  @Kenichan I dived many times for the ball. Man...  \n",
       "2        ElleCTF    my whole body feels itchy and like its on fire   \n",
       "3         Karoli  @nationwideclass no, it's not behaving at all....  \n",
       "4       joy_wolf                      @Kwesidei not the whole crew   "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>@Kwesidei not the whole crew</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Sentiment                                              Tweet\n",
       "0          0  is upset that he can't update his Facebook by ...\n",
       "1          0  @Kenichan I dived many times for the ball. Man...\n",
       "2          0    my whole body feels itchy and like its on fire \n",
       "3          0  @nationwideclass no, it's not behaving at all....\n",
       "4          0                      @Kwesidei not the whole crew "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df[[\"Sentiment\", \"Tweet\"]]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Dell\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Dell\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Dell\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\Dell\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk \n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_sentence(sentence):\n",
    "    # Convert to lowercase\n",
    "    sentence = sentence.lower()\n",
    "    \n",
    "    # Tokenize the sentence\n",
    "    tokens = word_tokenize(sentence)\n",
    "    \n",
    "    # Remove punctuation\n",
    "    tokens = [word for word in tokens if word.isalnum()]\n",
    "    \n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    \n",
    "    # Lemmatize\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    \n",
    "    return tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Tweet\"] = df[\"Tweet\"].apply(clean_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[upset, ca, update, facebook, texting, might, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>[kenichan, dived, many, time, ball, managed, s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>[whole, body, feel, itchy, like, fire]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>[nationwideclass, behaving, mad, ca, see]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>[kwesidei, whole, crew]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Sentiment                                              Tweet\n",
       "0          0  [upset, ca, update, facebook, texting, might, ...\n",
       "1          0  [kenichan, dived, many, time, ball, managed, s...\n",
       "2          0             [whole, body, feel, itchy, like, fire]\n",
       "3          0          [nationwideclass, behaving, mad, ca, see]\n",
       "4          0                            [kwesidei, whole, crew]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary length: 562182\n"
     ]
    }
   ],
   "source": [
    "# Build a vocabulary\n",
    "from collections import Counter\n",
    "\n",
    "def build_vocab(sentences):\n",
    "    all_tokens = [token for sentence in sentences for token in sentence]\n",
    "    token_counts = Counter(all_tokens)\n",
    "    vocab = {token: idx + 1 for idx, (token, _) in enumerate(token_counts.items())}\n",
    "    vocab['<PAD>'] = 0\n",
    "    return vocab\n",
    "\n",
    "vocab = build_vocab(sentences= df[\"Tweet\"])\n",
    "\n",
    "print(f\"Vocabulary length: {len(vocab)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert sentences to sequences of indices\n",
    "def sentence_to_indices(sentence, vocab):\n",
    "    return [vocab[token] for token in sentence]\n",
    "\n",
    "df[\"Tensor_Tweets\"] = df[\"Tweet\"].apply(lambda x : sentence_to_indices(x, vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Tweet</th>\n",
       "      <th>Tensor_Tweets</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[upset, ca, update, facebook, texting, might, ...</td>\n",
       "      <td>[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>[kenichan, dived, many, time, ball, managed, s...</td>\n",
       "      <td>[13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>[whole, body, feel, itchy, like, fire]</td>\n",
       "      <td>[24, 25, 26, 27, 28, 29]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>[nationwideclass, behaving, mad, ca, see]</td>\n",
       "      <td>[30, 31, 32, 2, 33]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>[kwesidei, whole, crew]</td>\n",
       "      <td>[34, 24, 35]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Sentiment                                              Tweet  \\\n",
       "0          0  [upset, ca, update, facebook, texting, might, ...   \n",
       "1          0  [kenichan, dived, many, time, ball, managed, s...   \n",
       "2          0             [whole, body, feel, itchy, like, fire]   \n",
       "3          0          [nationwideclass, behaving, mad, ca, see]   \n",
       "4          0                            [kwesidei, whole, crew]   \n",
       "\n",
       "                                  Tensor_Tweets  \n",
       "0       [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]  \n",
       "1  [13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23]  \n",
       "2                      [24, 25, 26, 27, 28, 29]  \n",
       "3                           [30, 31, 32, 2, 33]  \n",
       "4                                  [34, 24, 35]  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of training sample is (1359999,)\n",
      "Shape of testing sample  is (240000,)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X,Y = df[\"Tensor_Tweets\"].values, df[\"Sentiment\"].values \n",
    "\n",
    "X_train,X_test,Y_train,Y_test = train_test_split(X, Y, test_size = 0.15)\n",
    "\n",
    "print(f'Shape of training sample is {X_train.shape}')\n",
    "print(f'Shape of testing sample  is {X_test.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SENTENCE_LENGTH = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Padded sequence shape torch.Size([1599999, 50])\n"
     ]
    }
   ],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch \n",
    "\n",
    "def pad_sentences(sentences):\n",
    "    sequences = [torch.tensor(sentence) for sentence in sentences] \n",
    "    return pad_sequence(sequences, padding_value = 0)\n",
    "\n",
    "X = pad_sentences(df[\"Tensor_Tweets\"])\n",
    "X = X.T\n",
    "print(f\"Padded sequence shape {X.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 0, 0,  ..., 1, 1, 1], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "Y = torch.tensor(df[\"Sentiment\"], dtype = torch.int64)\n",
    "\n",
    "Y = Y/4 \n",
    "Y = Y.int()\n",
    "print(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader, random_split\n",
    "\n",
    "dataset = TensorDataset(X, Y)\n",
    "\n",
    "train_size = int(0.85 * len(dataset))  # 80% for training\n",
    "test_size = len(dataset) - train_size  # 20% for testing\n",
    "\n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataLoader for each subset\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Using LSTM for Sentiment Analysis\n",
    "\n",
    "LSTMs are a very popular form of Recurrent Neural Networks that uses a Constant Error Carousel (CEC) to solve the problem of vanishing gradient that is common in the RNNs. By using three types of gates, LSTMs are able to control the reading, writing and updating of the values. Here, we use one architecture of LSTMs to estimate the efficiency on the Twitter Sentiment Analysis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn \n",
    "\n",
    "class SentimentAnalysisLSTM(nn.Module):\n",
    "    \n",
    "    def __init__(self,\n",
    "                 vocab_size, \n",
    "                 embedding_dim, \n",
    "                 hidden_dim, \n",
    "                 output_dim, \n",
    "                 n_layers = 3, \n",
    "                 bidirectional = False, \n",
    "                 dropout = 0.3):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.__embed = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.__lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers = n_layers, \n",
    "                              bidirectional = bidirectional,\n",
    "                              dropout = dropout, \n",
    "                              batch_first = True) \n",
    "        \n",
    "        linear_outputs = hidden_dim\n",
    "        if bidirectional:\n",
    "            linear_outputs += hidden_dim\n",
    "            \n",
    "        self.__fc = nn.Linear(\n",
    "            linear_outputs, output_dim\n",
    "        )\n",
    "        self.__bidirectional = bidirectional\n",
    "        \n",
    "        self.__dropout = nn.Dropout(p = dropout)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        embedded = self.__embed(x)\n",
    "        \n",
    "        lstm_output, (hidden, cell) = self.__lstm(embedded) \n",
    "        \n",
    "        if self.__bidirectional:\n",
    "            hidden = self.__dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1))\n",
    "        else: \n",
    "            hidden = self.__dropout(hidden[-1,:,:])\n",
    "        \n",
    "        output = self.__fc(hidden) \n",
    "        return output\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "LSTMClassifier = SentimentAnalysisLSTM(\n",
    "    vocab_size = len(vocab),\n",
    "    embedding_dim = 100, \n",
    "    hidden_dim = 256,\n",
    "    output_dim = 1,\n",
    "    n_layers = 2,\n",
    "    bidirectional = True, \n",
    "    dropout = 0.3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "optimizer = torch.optim.Adam(LSTMClassifier.parameters(), lr = 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters: 58528857\n"
     ]
    }
   ],
   "source": [
    "count = 0 \n",
    "for p in LSTMClassifier.parameters():\n",
    "    count += p.numel()\n",
    "    \n",
    "print(f\"Total number of parameters: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SentimentAnalysisLSTM(\n",
       "  (_SentimentAnalysisLSTM__embed): Embedding(562182, 100)\n",
       "  (_SentimentAnalysisLSTM__lstm): LSTM(100, 256, num_layers=2, batch_first=True, dropout=0.3, bidirectional=True)\n",
       "  (_SentimentAnalysisLSTM__fc): Linear(in_features=512, out_features=1, bias=True)\n",
       "  (_SentimentAnalysisLSTM__dropout): Dropout(p=0.3, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LSTMClassifier.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------\n",
      "Epoch $1\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10625/10625 [12:31<00:00, 14.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining loss:  0.472296\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1875/1875 [00:18<00:00, 99.12it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTest Loss: 0.4444\n",
      "\tAccuracy: 0.7907\n",
      "-------------------------\n",
      "Epoch $2\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10625/10625 [09:24<00:00, 18.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining loss:  0.412780\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1875/1875 [00:20<00:00, 89.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTest Loss: 0.4368\n",
      "\tAccuracy: 0.7959\n",
      "-------------------------\n",
      "Epoch $3\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10625/10625 [09:35<00:00, 18.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining loss:  0.365751\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1875/1875 [00:20<00:00, 91.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTest Loss: 0.4504\n",
      "\tAccuracy: 0.7931\n",
      "-------------------------\n",
      "Epoch $4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10625/10625 [09:32<00:00, 18.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining loss:  0.315260\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1875/1875 [00:20<00:00, 89.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTest Loss: 0.4839\n",
      "\tAccuracy: 0.7876\n",
      "-------------------------\n",
      "Epoch $5\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10625/10625 [09:28<00:00, 18.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining loss:  0.266740\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1875/1875 [00:20<00:00, 93.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTest Loss: 0.5184\n",
      "\tAccuracy: 0.7821\n",
      "-------------------------\n",
      "Epoch $6\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10625/10625 [09:22<00:00, 18.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining loss:  0.225637\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1875/1875 [00:20<00:00, 93.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTest Loss: 0.5757\n",
      "\tAccuracy: 0.7761\n",
      "-------------------------\n",
      "Epoch $7\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10625/10625 [09:22<00:00, 18.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining loss:  0.194305\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1875/1875 [00:20<00:00, 91.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTest Loss: 0.6094\n",
      "\tAccuracy: 0.7713\n",
      "-------------------------\n",
      "Epoch $8\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10625/10625 [09:20<00:00, 18.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining loss:  0.171337\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1875/1875 [00:19<00:00, 94.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTest Loss: 0.6806\n",
      "\tAccuracy: 0.7662\n",
      "-------------------------\n",
      "Epoch $9\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10625/10625 [09:22<00:00, 18.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining loss:  0.154582\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1875/1875 [00:20<00:00, 93.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTest Loss: 0.7134\n",
      "\tAccuracy: 0.7653\n",
      "-------------------------\n",
      "Epoch $10\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10625/10625 [09:27<00:00, 18.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining loss:  0.142447\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1875/1875 [00:20<00:00, 92.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTest Loss: 0.7205\n",
      "\tAccuracy: 0.7632\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm \n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print(\"-------------------------\")\n",
    "    print(f\"Epoch ${epoch + 1}\")\n",
    "    print()\n",
    "    \n",
    "    LSTMClassifier.train()\n",
    "    train_loss = 0.0\n",
    "    \n",
    "    for batch in tqdm(train_loader):\n",
    "        text, sentiment = batch \n",
    "        text = text.to(device)\n",
    "        sentiment = sentiment.to(device).float()\n",
    "        \n",
    "        output = LSTMClassifier(text).squeeze(1)\n",
    "        \n",
    "        training_loss = criterion(output, sentiment)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        training_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += training_loss.item()\n",
    "    \n",
    "    print(f\"\\tTraining loss: {train_loss/len(train_loader): .6f}\")\n",
    "    \n",
    "    LSTMClassifier.eval()\n",
    "    \n",
    "    test_loss = 0\n",
    "    correct_predictions = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_loader):\n",
    "            texts, labels = batch\n",
    "            texts, labels = texts.to(device), labels.to(device).float()\n",
    "        \n",
    "            predictions = LSTMClassifier(texts).squeeze(1)\n",
    "            loss = criterion(predictions, labels)\n",
    "            test_loss += loss.item()\n",
    "        \n",
    "            predicted_labels = torch.round(torch.sigmoid(predictions))\n",
    "            correct_predictions += (predicted_labels == labels).sum().item()\n",
    "    \n",
    "    print(f'\\tTest Loss: {test_loss/len(test_loader):.4f}')\n",
    "    print(f'\\tAccuracy: {correct_predictions / len(test_loader.dataset):.4f}')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1875/1875 [00:19<00:00, 94.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tAccuracy on Test Data : 0.7632\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10625/10625 [01:57<00:00, 90.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tAccuracy on Training Data : 0.9652\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Print final training and testing accuracy \n",
    "LSTMClassifier.eval()\n",
    "correct_predictions = 0\n",
    "    \n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(test_loader):\n",
    "        texts, labels = batch\n",
    "        texts, labels = texts.to(device), labels.to(device).float()\n",
    "        \n",
    "        predictions = LSTMClassifier(texts).squeeze(1)\n",
    "        loss = criterion(predictions, labels)\n",
    "        \n",
    "        predicted_labels = torch.round(torch.sigmoid(predictions))\n",
    "        correct_predictions += (predicted_labels == labels).sum().item()\n",
    "\n",
    "print(f'\\tAccuracy on Test Data : {correct_predictions / len(test_loader.dataset):.4f}')\n",
    "\n",
    "correct_predictions = 0\n",
    "    \n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(train_loader):\n",
    "        texts, labels = batch\n",
    "        texts, labels = texts.to(device), labels.to(device).float()\n",
    "        \n",
    "        predictions = LSTMClassifier(texts).squeeze(1)\n",
    "        loss = criterion(predictions, labels)\n",
    "        \n",
    "        predicted_labels = torch.round(torch.sigmoid(predictions))\n",
    "        correct_predictions += (predicted_labels == labels).sum().item()\n",
    "\n",
    "print(f'\\tAccuracy on Training Data : {correct_predictions / len(train_loader.dataset):.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that LSTMs achieve an accuracy of 76.32 % on the test data and 96.52% on the training data. This suggests that LSTMs are pretty much able to get the best out of it. Yet we want to improve a bit..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 : Using GRUs\n",
    "\n",
    "GRUs are a simplified version of LSTMs that employs only two gates : the input and the reset gate. It is used for the instances when we need to minimize the number of parameters as the performance tradeoff between LSTMs and GRUs is very less in most cases, while GRUs tend to provide enhanced speed of computation due to lesser number of variables in it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn \n",
    "\n",
    "class SentimentAnalysisGRU(nn.Module):\n",
    "    def __init__(self, \n",
    "                 vocab_size, \n",
    "                 embedding_dim, \n",
    "                 hidden_dim, \n",
    "                 output_dim, \n",
    "                 n_layers = 3, \n",
    "                 bidirectional = True, \n",
    "                 dropout = 0.3):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = nn.GRU(embedding_dim, hidden_dim, num_layers=n_layers, bidirectional=bidirectional, dropout=dropout, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim * 2 if bidirectional else hidden_dim, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        gru_out, hidden = self.gru(embedded)\n",
    "        \n",
    "        if self.gru.bidirectional:\n",
    "            hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1))\n",
    "        else:\n",
    "            hidden = self.dropout(hidden[-1,:,:])\n",
    "            \n",
    "        output = self.fc(hidden)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "GRUClassifier = SentimentAnalysisGRU(\n",
    "    vocab_size = len(vocab),\n",
    "    embedding_dim = 100, \n",
    "    hidden_dim = 256,\n",
    "    output_dim = 1,\n",
    "    n_layers = 2,\n",
    "    bidirectional = True, \n",
    "    dropout = 0.3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "optimizer = torch.optim.Adam(GRUClassifier.parameters(), lr = 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters: 57951321\n"
     ]
    }
   ],
   "source": [
    "count = 0 \n",
    "for p in GRUClassifier.parameters():\n",
    "    count += p.numel()\n",
    "    \n",
    "print(f\"Total number of parameters: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SentimentAnalysisGRU(\n",
       "  (embedding): Embedding(562182, 100)\n",
       "  (gru): GRU(100, 256, num_layers=2, batch_first=True, dropout=0.3, bidirectional=True)\n",
       "  (fc): Linear(in_features=512, out_features=1, bias=True)\n",
       "  (dropout): Dropout(p=0.3, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GRUClassifier.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------\n",
      "Epoch $1\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10625/10625 [09:01<00:00, 19.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining loss:  0.477222\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1875/1875 [00:22<00:00, 83.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTest Loss: 0.4499\n",
      "\tAccuracy: 0.7868\n",
      "-------------------------\n",
      "Epoch $2\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10625/10625 [08:48<00:00, 20.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining loss:  0.420558\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1875/1875 [00:22<00:00, 82.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTest Loss: 0.4455\n",
      "\tAccuracy: 0.7913\n",
      "-------------------------\n",
      "Epoch $3\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10625/10625 [09:16<00:00, 19.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining loss:  0.382931\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1875/1875 [00:19<00:00, 94.56it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTest Loss: 0.4602\n",
      "\tAccuracy: 0.7896\n",
      "-------------------------\n",
      "Epoch $4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10625/10625 [08:53<00:00, 19.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining loss:  0.347965\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1875/1875 [00:19<00:00, 94.27it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTest Loss: 0.4654\n",
      "\tAccuracy: 0.7846\n",
      "-------------------------\n",
      "Epoch $5\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10625/10625 [08:57<00:00, 19.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining loss:  0.318237\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1875/1875 [00:21<00:00, 86.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTest Loss: 0.4828\n",
      "\tAccuracy: 0.7787\n",
      "-------------------------\n",
      "Epoch $6\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10625/10625 [09:04<00:00, 19.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining loss:  0.297245\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1875/1875 [00:19<00:00, 95.18it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTest Loss: 0.5038\n",
      "\tAccuracy: 0.7774\n",
      "-------------------------\n",
      "Epoch $7\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10625/10625 [08:52<00:00, 19.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining loss:  0.283876\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1875/1875 [00:21<00:00, 88.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTest Loss: 0.5206\n",
      "\tAccuracy: 0.7759\n",
      "-------------------------\n",
      "Epoch $8\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10625/10625 [08:51<00:00, 20.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining loss:  0.277227\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1875/1875 [00:19<00:00, 94.39it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTest Loss: 0.5288\n",
      "\tAccuracy: 0.7696\n",
      "-------------------------\n",
      "Epoch $9\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10625/10625 [08:46<00:00, 20.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining loss:  0.277027\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1875/1875 [00:19<00:00, 94.37it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTest Loss: 0.5632\n",
      "\tAccuracy: 0.7717\n",
      "-------------------------\n",
      "Epoch $10\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10625/10625 [09:03<00:00, 19.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining loss:  0.279249\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1875/1875 [00:21<00:00, 88.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTest Loss: 0.5156\n",
      "\tAccuracy: 0.7685\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm \n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print(\"-------------------------\")\n",
    "    print(f\"Epoch ${epoch + 1}\")\n",
    "    print()\n",
    "    \n",
    "    GRUClassifier.train()\n",
    "    train_loss = 0.0\n",
    "    \n",
    "    for batch in tqdm(train_loader):\n",
    "        text, sentiment = batch \n",
    "        text = text.to(device)\n",
    "        sentiment = sentiment.to(device).float()\n",
    "        \n",
    "        output = GRUClassifier(text).squeeze(1)\n",
    "        \n",
    "        training_loss = criterion(output, sentiment)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        training_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += training_loss.item()\n",
    "    \n",
    "    print(f\"\\tTraining loss: {train_loss/len(train_loader): .6f}\")\n",
    "    \n",
    "    LSTMClassifier.eval()\n",
    "    \n",
    "    test_loss = 0\n",
    "    correct_predictions = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_loader):\n",
    "            texts, labels = batch\n",
    "            texts, labels = texts.to(device), labels.to(device).float()\n",
    "        \n",
    "            predictions = GRUClassifier(texts).squeeze(1)\n",
    "            loss = criterion(predictions, labels)\n",
    "            test_loss += loss.item()\n",
    "        \n",
    "            predicted_labels = torch.round(torch.sigmoid(predictions))\n",
    "            correct_predictions += (predicted_labels == labels).sum().item()\n",
    "    \n",
    "    print(f'\\tTest Loss: {test_loss/len(test_loader):.4f}')\n",
    "    print(f'\\tAccuracy: {correct_predictions / len(test_loader.dataset):.4f}')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1875/1875 [00:21<00:00, 87.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tAccuracy on Test Data : 0.7699\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10625/10625 [01:55<00:00, 91.64it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tAccuracy on Training Data : 0.8933\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Print final training and testing accuracy \n",
    "GRUClassifier.eval()\n",
    "correct_predictions = 0\n",
    "    \n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(test_loader):\n",
    "        texts, labels = batch\n",
    "        texts, labels = texts.to(device), labels.to(device).float()\n",
    "        \n",
    "        predictions = GRUClassifier(texts).squeeze(1)\n",
    "        loss = criterion(predictions, labels)\n",
    "        \n",
    "        predicted_labels = torch.round(torch.sigmoid(predictions))\n",
    "        correct_predictions += (predicted_labels == labels).sum().item()\n",
    "\n",
    "print(f'\\tAccuracy on Test Data : {correct_predictions / len(test_loader.dataset):.4f}')\n",
    "\n",
    "correct_predictions = 0\n",
    "    \n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(train_loader):\n",
    "        texts, labels = batch\n",
    "        texts, labels = texts.to(device), labels.to(device).float()\n",
    "        \n",
    "        predictions = GRUClassifier(texts).squeeze(1)\n",
    "        loss = criterion(predictions, labels)\n",
    "        \n",
    "        predicted_labels = torch.round(torch.sigmoid(predictions))\n",
    "        correct_predictions += (predicted_labels == labels).sum().item()\n",
    "\n",
    "print(f'\\tAccuracy on Training Data : {correct_predictions / len(train_loader.dataset):.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that GRUs achieve an accuracy of 76.99 % on the test data and 89.33% on the training data. This suggests that GRU performance are better than LSTMs with a difference of over 0.67% in the test data. Also, GRUs have a bit faster training due to lesser parameters and simpler iteration. \n",
    "\n",
    "Each iteration of LSTM took around 9 min 20s while each iteration of GRUs was done in approximately 8 min 55s. This indicates a difference of 30s, which is significantly lower for GRUs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. CNN Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SentimentAnalysisCNN(nn.Module):\n",
    "    def __init__(self, \n",
    "                 vocab_size, \n",
    "                 embedding_dim, \n",
    "                 num_filters, \n",
    "                 filter_sizes, \n",
    "                 output_dim = 1, \n",
    "                 dropout = 0.3):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.__embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.__convs = nn.ModuleList([\n",
    "            nn.Conv2d(in_channels=1, out_channels=num_filters, kernel_size=(fs, embedding_dim)) \n",
    "            for fs in filter_sizes\n",
    "        ])\n",
    "        self.__fc = nn.Linear(len(filter_sizes) * num_filters, output_dim)\n",
    "        self.__dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        embedded = self.__embedding(x).unsqueeze(1)  # Add channel dimension for convolution\n",
    "        conved = [F.relu(conv(embedded)).squeeze(3) for conv in self.__convs]  # Convolution\n",
    "        pooled = [F.max_pool1d(conv, conv.shape[2]).squeeze(2) for conv in conved]  # Max pooling\n",
    "        cat = self.__dropout(torch.cat(pooled, dim=1))  # Concatenate and apply dropout\n",
    "        output = self.__fc(cat)  # Fully connected layer\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "vocab_size = len(vocab)  # Plus 1 for padding token\n",
    "embedding_dim = 100\n",
    "num_filters = 100\n",
    "filter_sizes = [3, 5, 6, 8]  # Window sizes for convolution\n",
    "output_dim = 1  # Binary classification\n",
    "dropout = 0.5\n",
    "\n",
    "CNNClassifier = SentimentAnalysisCNN(vocab_size, embedding_dim, num_filters, filter_sizes, output_dim, dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters: 56338801\n"
     ]
    }
   ],
   "source": [
    "count = 0 \n",
    "for p in CNNClassifier.parameters():\n",
    "    count += p.numel()\n",
    "    \n",
    "print(f\"Total number of parameters: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "optimizer = torch.optim.Adam(CNNClassifier.parameters(), lr = 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SentimentAnalysisCNN(\n",
       "  (_SentimentAnalysisCNN__embedding): Embedding(562182, 100)\n",
       "  (_SentimentAnalysisCNN__convs): ModuleList(\n",
       "    (0): Conv2d(1, 100, kernel_size=(3, 100), stride=(1, 1))\n",
       "    (1): Conv2d(1, 100, kernel_size=(4, 100), stride=(1, 1))\n",
       "    (2): Conv2d(1, 100, kernel_size=(5, 100), stride=(1, 1))\n",
       "  )\n",
       "  (_SentimentAnalysisCNN__fc): Linear(in_features=300, out_features=1, bias=True)\n",
       "  (_SentimentAnalysisCNN__dropout): Dropout(p=0.5, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CNNClassifier.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------\n",
      "Epoch $1\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10625/10625 [05:27<00:00, 32.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining loss:  0.449093\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1875/1875 [00:05<00:00, 356.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTest Loss: 0.4509\n",
      "\tAccuracy: 0.7874\n",
      "-------------------------\n",
      "Epoch $2\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10625/10625 [05:33<00:00, 31.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining loss:  0.423738\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1875/1875 [00:05<00:00, 348.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTest Loss: 0.4524\n",
      "\tAccuracy: 0.7885\n",
      "-------------------------\n",
      "Epoch $3\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10625/10625 [05:31<00:00, 32.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining loss:  0.399821\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1875/1875 [00:05<00:00, 344.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTest Loss: 0.4622\n",
      "\tAccuracy: 0.7877\n",
      "-------------------------\n",
      "Epoch $4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10625/10625 [05:31<00:00, 32.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining loss:  0.378164\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1875/1875 [00:05<00:00, 344.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTest Loss: 0.4841\n",
      "\tAccuracy: 0.7811\n",
      "-------------------------\n",
      "Epoch $5\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10625/10625 [05:30<00:00, 32.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining loss:  0.357297\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1875/1875 [00:05<00:00, 344.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTest Loss: 0.4960\n",
      "\tAccuracy: 0.7835\n",
      "-------------------------\n",
      "Epoch $6\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10625/10625 [05:30<00:00, 32.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining loss:  0.337871\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1875/1875 [00:05<00:00, 361.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTest Loss: 0.5122\n",
      "\tAccuracy: 0.7809\n",
      "-------------------------\n",
      "Epoch $7\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10625/10625 [05:32<00:00, 31.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining loss:  0.319344\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1875/1875 [00:05<00:00, 342.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTest Loss: 0.5254\n",
      "\tAccuracy: 0.7777\n",
      "-------------------------\n",
      "Epoch $8\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10625/10625 [05:29<00:00, 32.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining loss:  0.302163\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1875/1875 [00:05<00:00, 336.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTest Loss: 0.5569\n",
      "\tAccuracy: 0.7719\n",
      "-------------------------\n",
      "Epoch $9\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10625/10625 [05:30<00:00, 32.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining loss:  0.285816\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1875/1875 [00:07<00:00, 267.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTest Loss: 0.5898\n",
      "\tAccuracy: 0.7727\n",
      "-------------------------\n",
      "Epoch $10\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10625/10625 [05:30<00:00, 32.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining loss:  0.270289\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1875/1875 [00:05<00:00, 354.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTest Loss: 0.6138\n",
      "\tAccuracy: 0.7692\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm \n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print(\"-------------------------\")\n",
    "    print(f\"Epoch ${epoch + 1}\")\n",
    "    print()\n",
    "    \n",
    "    CNNClassifier.train()\n",
    "    train_loss = 0.0\n",
    "    \n",
    "    for batch in tqdm(train_loader):\n",
    "        text, sentiment = batch \n",
    "        text = text.to(device)\n",
    "        sentiment = sentiment.to(device).float()\n",
    "        \n",
    "        output = CNNClassifier(text).squeeze(1)\n",
    "        \n",
    "        training_loss = criterion(output, sentiment)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        training_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += training_loss.item()\n",
    "    \n",
    "    print(f\"\\tTraining loss: {train_loss/len(train_loader): .6f}\")\n",
    "    \n",
    "    CNNClassifier.eval()\n",
    "    \n",
    "    test_loss = 0\n",
    "    correct_predictions = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_loader):\n",
    "            texts, labels = batch\n",
    "            texts, labels = texts.to(device), labels.to(device).float()\n",
    "        \n",
    "            predictions = CNNClassifier(texts).squeeze(1)\n",
    "            loss = criterion(predictions, labels)\n",
    "            test_loss += loss.item()\n",
    "        \n",
    "            predicted_labels = torch.round(torch.sigmoid(predictions))\n",
    "            correct_predictions += (predicted_labels == labels).sum().item()\n",
    "    \n",
    "    print(f'\\tTest Loss: {test_loss/len(test_loader):.4f}')\n",
    "    print(f'\\tAccuracy: {correct_predictions / len(test_loader.dataset):.4f}')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1875/1875 [00:05<00:00, 357.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tAccuracy on Test Data : 0.7692\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10625/10625 [00:30<00:00, 350.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tAccuracy on Training Data : 0.9113\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Print final training and testing accuracy \n",
    "CNNClassifier.eval()\n",
    "correct_predictions = 0\n",
    "    \n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(test_loader):\n",
    "        texts, labels = batch\n",
    "        texts, labels = texts.to(device), labels.to(device).float()\n",
    "        \n",
    "        predictions = CNNClassifier(texts).squeeze(1)\n",
    "        loss = criterion(predictions, labels)\n",
    "        \n",
    "        predicted_labels = torch.round(torch.sigmoid(predictions))\n",
    "        correct_predictions += (predicted_labels == labels).sum().item()\n",
    "\n",
    "print(f'\\tAccuracy on Test Data : {correct_predictions / len(test_loader.dataset):.4f}')\n",
    "\n",
    "correct_predictions = 0\n",
    "    \n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(train_loader):\n",
    "        texts, labels = batch\n",
    "        texts, labels = texts.to(device), labels.to(device).float()\n",
    "        \n",
    "        predictions = CNNClassifier(texts).squeeze(1)\n",
    "        loss = criterion(predictions, labels)\n",
    "        \n",
    "        predicted_labels = torch.round(torch.sigmoid(predictions))\n",
    "        correct_predictions += (predicted_labels == labels).sum().item()\n",
    "\n",
    "print(f'\\tAccuracy on Training Data : {correct_predictions / len(train_loader.dataset):.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is only a slight improvement to 76.92% on the test data set yet the accuracy on training data is only 91.13%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {'C': [0.1, 1, 10, 100], 'kernel': ['linear', 'rbf']}\n",
    "grid_search = GridSearchCV(SVC(), param_grid, cv=5)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "best_model = grid_search.best_estimator_\n",
    "print(\"Best Parameters:\", grid_search.best_params_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
